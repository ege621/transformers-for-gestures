{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b275df22",
   "metadata": {
    "id": "b275df22"
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kznmlw3C6khr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 421,
     "status": "ok",
     "timestamp": 1662989801022,
     "user": {
      "displayName": "Ege Keskin",
      "userId": "01820174062081175694"
     },
     "user_tz": -180
    },
    "id": "kznmlw3C6khr",
    "outputId": "68d8437e-30e3-42a0-ed00-31469ab0dd00"
   },
   "outputs": [],
   "source": [
    "%cd /content/drive/MyDrive/Action2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eVM8CdOM6s6f",
   "metadata": {
    "id": "eVM8CdOM6s6f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install opencv-python mediapipe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc01baa",
   "metadata": {
    "executionInfo": {
     "elapsed": 376,
     "status": "ok",
     "timestamp": 1662990255784,
     "user": {
      "displayName": "Ege Keskin",
      "userId": "01820174062081175694"
     },
     "user_tz": -180
    },
    "id": "9dc01baa",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split #use this to split the data \n",
    "from tensorflow.keras.callbacks import EarlyStopping #use this to abort the training process early\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.metrics import classification_report\n",
    "import mlflow.tensorflow\n",
    "mlflow.tensorflow.autolog()\n",
    "#do not run the below line if you only have one GPU\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" # use second GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2aefcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What version of Python do you have?\n",
    "import sys\n",
    "import tensorflow.keras\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import tensorflow as tf\n",
    "import platform\n",
    "\n",
    "print(f\"Python Platform: {platform.platform()}\")\n",
    "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {tensorflow.keras.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Scikit-Learn {sk.__version__}\")\n",
    "gpu = len(tf.config.list_physical_devices('GPU'))>0\n",
    "print(\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfeb3d0",
   "metadata": {
    "id": "7bfeb3d0"
   },
   "source": [
    "# Initialize mediapipe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fab356",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1662990038298,
     "user": {
      "displayName": "Ege Keskin",
      "userId": "01820174062081175694"
     },
     "user_tz": -180
    },
    "id": "24fab356",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mpHands = mp.solutions.hands #initiate the mpHands object with mediapipe\n",
    "\n",
    "hands = mpHands.Hands(static_image_mode=False, #set prediction parameters \n",
    "                      max_num_hands=2,\n",
    "                      min_detection_confidence=0.5,\n",
    "                      min_tracking_confidence=0.5)\n",
    "mpDraw = mp.solutions.drawing_utils #initiate the drawing object vor visualization\n",
    "\n",
    "#we specifically use the Hands part of the mediapipe detection pipeline, since we do not need face pose etc.\n",
    "#the output of mpHands.Hands is a dictionary consisting of 0 and 1, meaning the left hand and the right hand in order.\n",
    "#each hand consists of 21 keypoints, each containing 3 pixels (x,y,z) values.\n",
    "#the values are normalized according to the image width and height by default.\n",
    "\n",
    "#this function flattens all the data to an array for left and right hand and joins them together.\n",
    "#since our model will run based on two hands, we have to make sure we somehow fill all of the 126 features\n",
    "#our model requires.\n",
    "#to do this, we extract the first and last sub dictionary of the mediapipe output.\n",
    "#by doing this, if only one hand is present, the missing hand data will be duplicated as the present hand data.\n",
    "#the model will throw a shape mismatch error if all of the features are not present. \n",
    "def extract_keypoints(results):   \n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.multi_hand_landmarks[0].landmark]).flatten() if results.multi_hand_landmarks else np.zeros(21*3)   \n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.multi_hand_landmarks[-1].landmark]).flatten() if results.multi_hand_landmarks else np.zeros(21*3)   \n",
    "    \n",
    "    return np.concatenate([lh, rh])\n",
    "\n",
    "\n",
    "DATA_PATH = os.path.join('HANDS_DATA') #path for collecting data\n",
    "actions = np.array(['open', 'select' , 'close' , 'confirm' , 'reject' , #list of actions or classes to be predicted\n",
    "                    'increasevol' , 'decreasevol' , 'next' , 'previous'])\n",
    "\n",
    "n_classes = len(actions) #how many classes are there"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SNdNQOYF7QdJ",
   "metadata": {
    "id": "SNdNQOYF7QdJ"
   },
   "source": [
    "Initialize precision and recall functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "I3AKt5nm64sh",
   "metadata": {
    "executionInfo": {
     "elapsed": 905,
     "status": "ok",
     "timestamp": 1662989976033,
     "user": {
      "displayName": "Ege Keskin",
      "userId": "01820174062081175694"
     },
     "user_tz": -180
    },
    "id": "I3AKt5nm64sh"
   },
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d0b814",
   "metadata": {
    "id": "76d0b814"
   },
   "source": [
    "# Import the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204bd65d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1966,
     "status": "ok",
     "timestamp": 1662989984166,
     "user": {
      "displayName": "Ege Keskin",
      "userId": "01820174062081175694"
     },
     "user_tz": -180
    },
    "id": "204bd65d",
    "outputId": "d83e3be6-7576-4bb7-a9ef-fc7d01ad122d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#use this for augmented data\n",
    "X = np.load('dataset/ege180_kubi60_ceren30_X.npy')\n",
    "y = np.load('dataset/ege180_kubi60_ceren30_y.npy')\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47db565f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split our dataset to test and train with the ratio test_size.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(\"Shape of whole sequences as giant array: {}\".format(np.array(X).shape))\n",
    "print(\"Shape of X_train: {}\".format(X_train.shape))\n",
    "print(\"Shape of X_test: {}\".format(X_test.shape))\n",
    "print(\"Shape of y_train: {}\".format(y_train.shape))\n",
    "print(\"Shape of y_test: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67dd48f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4090,
     "status": "ok",
     "timestamp": 1662990008646,
     "user": {
      "displayName": "Ege Keskin",
      "userId": "01820174062081175694"
     },
     "user_tz": -180
    },
    "id": "c67dd48f",
    "outputId": "19988140-ffe9-45cb-b20f-7f93aa363dcf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#split our dataset to test and train with the ratio test_size.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(\"Shape of whole sequences as giant array: {}\".format(np.array(X).shape))\n",
    "print(\"Shape of X_train: {}\".format(X_train.shape))\n",
    "print(\"Shape of X_test: {}\".format(X_test.shape))\n",
    "print(\"Shape of y_train: {}\".format(y_train.shape))\n",
    "print(\"Shape of y_test: {}\".format(y_test.shape))\n",
    "\n",
    "\n",
    "#augment the training set\n",
    "iterations = 10\n",
    "mu, sigma = 0, 0.1\n",
    "X_new = np.zeros((X_train.shape[0],X_train.shape[1],X_train.shape[2]))\n",
    "y_new = np.zeros((y_train.shape[0],y_train.shape[1]))\n",
    "\n",
    "for i in range(iterations):\n",
    "    np.random.seed(i)\n",
    "    noise = np.random.normal(mu, sigma, [X_train.shape[0],X_train.shape[1],X_train.shape[2]])\n",
    "    Xt = np.multiply(noise,X_train) + X_train\n",
    "    X_new = np.concatenate([X_new, Xt], axis = 0)\n",
    "    print(\"Done with the {}th iteration\".format(i))  \n",
    "X_new = X_new[X_train.shape[0]:]    \n",
    "\n",
    "for i in range(iterations):\n",
    "    y_new = np.concatenate([y_train, y_new], 0)\n",
    "\n",
    "y_new = y_new[~np.all(y_new == 0, axis=1)] #remove rows that only contain 0's\n",
    "\n",
    "X_train = X_new\n",
    "y_train = y_new\n",
    "\n",
    "#augment the test set (this is more agressive)\n",
    "mu, sigma = 0, 0.105\n",
    "X_new = np.zeros((X_test.shape[0],X_test.shape[1],X_test.shape[2]))\n",
    "y_new = np.zeros((y_test.shape[0],y_test.shape[1]))\n",
    "\n",
    "for i in range(iterations):\n",
    "    np.random.seed(i)\n",
    "    noise = np.random.normal(mu, sigma, [X_test.shape[0],X_test.shape[1],X_test.shape[2]])\n",
    "    Xt = np.multiply(noise,X_test) + X_test\n",
    "    X_new = np.concatenate([X_new, Xt], axis = 0)\n",
    "    print(\"Done with the {}th iteration\".format(i))  \n",
    "X_new = X_new[X_test.shape[0]:]    \n",
    "\n",
    "for i in range(iterations):\n",
    "    y_new = np.concatenate([y_test, y_new], 0)\n",
    "\n",
    "y_new = y_new[~np.all(y_new == 0, axis=1)] #remove rows that only contain 0's\n",
    "\n",
    "X_test = X_new\n",
    "y_test = y_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227701a3",
   "metadata": {
    "id": "227701a3"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6830bd2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1662990008647,
     "user": {
      "displayName": "Ege Keskin",
      "userId": "01820174062081175694"
     },
     "user_tz": -180
    },
    "id": "c6830bd2",
    "outputId": "81b057b9-7694-44df-fc41-6dead239ecb4"
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89079fe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 959,
     "status": "ok",
     "timestamp": 1662990047109,
     "user": {
      "displayName": "Ege Keskin",
      "userId": "01820174062081175694"
     },
     "user_tz": -180
    },
    "id": "d89079fe",
    "outputId": "5b750225-89db-449c-cd8a-c6ab336817aa",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#This is the main transformer block\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs) #layer normalization is applied to the input to ease training\n",
    "    x = layers.MultiHeadAttention(    #This is the attention layer. You can experiment with different parameters \n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x) #as a result, apply dropout \n",
    "    res = x + inputs #residual connection\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res) #feed the result to layer normalization\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x) #use 1d convolution with kernel size 1 to preserve original sequence length\n",
    "    x = layers.Dropout(dropout)(x) #apply dropout\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x) #we need a filter size of the feature count so we can calculate weights\n",
    "    return x + res\n",
    "\n",
    "\n",
    "def build_model( #construct the model \n",
    "    input_shape,\n",
    "    head_size, #head size for the attention block\n",
    "    num_heads,#number of heads for the attention block\n",
    "    ff_dim, #feed forward dimensions\n",
    "    num_transformer_blocks, #specifies how many times the transformer block is repeated\n",
    "    mlp_units, #multilayer perceptron units\n",
    "    dropout=0, #by default\n",
    "    mlp_dropout=0, #by default\n",
    "):\n",
    "    inputs = keras.Input(shape=input_shape) #take the input\n",
    "    x = inputs   \n",
    "    \n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout) #feed the input to the transformer block\n",
    "    \n",
    "    \n",
    "    #apply global average pooling for classification\n",
    "    #be aware of the data format argument. If channels_last is specified, the dataset should be as\n",
    "    # (batch size,sequence length,number of features)\n",
    "    # if specified as channels first, sequence length and number feature dimensions are swapped.\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_last\")(x) \n",
    "    \n",
    "    \n",
    "    \n",
    "    for dim in mlp_units: #create the dense layer\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = layers.Dense(n_classes, activation=\"softmax\")(x) #apply softmax for probability distribution\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "input_shape = X_train.shape[1:] #take the last two arguments of the input shape which are 30,126\n",
    "\n",
    "model = build_model( #these are all hyperparameters for the model complexity. General rule of thumb\n",
    "    #would be two start as low as possible for all of them except the dropouts and gradually increase them until overfitting\n",
    "    input_shape,\n",
    "    head_size=128,\n",
    "    num_heads=4,\n",
    "    ff_dim=32,\n",
    "    num_transformer_blocks=1,\n",
    "    mlp_units=[16], #32\n",
    "    mlp_dropout=0, #0.25\n",
    "    dropout=0, #0.25\n",
    ")\n",
    "\n",
    "model.compile( #use adam optimizer\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=5e-5), #learning rate is also a hyperparameter\n",
    "    metrics=[\"categorical_accuracy\"],\n",
    ")\n",
    "model.summary() #take a look at the model structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b30578b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "executionInfo": {
     "elapsed": 36953,
     "status": "error",
     "timestamp": 1662990096464,
     "user": {
      "displayName": "Ege Keskin",
      "userId": "01820174062081175694"
     },
     "user_tz": -180
    },
    "id": "7b30578b",
    "outputId": "a7b4b692-2444-46b6-d1a7-888f2d0e2d09"
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='loss', verbose=1, patience=5000) #if we do not see improvement on test accuracy for 10 epochs, stop training\n",
    "#start training. we assign this training to a variable so we can log the training graphs.\n",
    "#if you quit training early with ctrl+c, the variable is lost and you cannot plot the graphs.\n",
    "#history = model.fit(X_train, y_train, validation_split = 0.3 , epochs=200, verbose = 1, callbacks = [es])\n",
    "history = model.fit(X_train, y_train, validation_split = 0.2, epochs=100, verbose = 1, callbacks = [es])\n",
    "\n",
    "print(history.history.keys()) #see what can we plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ebb062",
   "metadata": {
    "id": "38ebb062"
   },
   "source": [
    "# Plotting training performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c389877",
   "metadata": {
    "id": "4c389877",
    "outputId": "f433ffda-83d2-4e87-b655-4e3c5e21b6d3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['categorical_accuracy'])\n",
    "plt.plot(history.history['val_categorical_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a7e4fe",
   "metadata": {
    "id": "12a7e4fe"
   },
   "source": [
    "# Test the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GeePCbzf74LO",
   "metadata": {
    "executionInfo": {
     "elapsed": 383,
     "status": "ok",
     "timestamp": 1662990149190,
     "user": {
      "displayName": "Ege Keskin",
      "userId": "01820174062081175694"
     },
     "user_tz": -180
    },
    "id": "GeePCbzf74LO"
   },
   "outputs": [],
   "source": [
    "model.load_weights('models/11EylulTransformer.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2565ede1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 601
    },
    "executionInfo": {
     "elapsed": 3396,
     "status": "ok",
     "timestamp": 1662990158396,
     "user": {
      "displayName": "Ege Keskin",
      "userId": "01820174062081175694"
     },
     "user_tz": -180
    },
    "id": "2565ede1",
    "outputId": "052140a4-5a2e-439d-d68a-5976fb92d070"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "y_pred = model.predict(X_test) #Test the model with our test set, since we have used cross validation\n",
    "\n",
    "cm = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=actions)\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "disp.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80174604",
   "metadata": {
    "id": "80174604",
    "outputId": "5d509719-371c-48a8-c214-58fb19fa18bd"
   },
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cTaIYB3T8LrY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3051,
     "status": "ok",
     "timestamp": 1662990266601,
     "user": {
      "displayName": "Ege Keskin",
      "userId": "01820174062081175694"
     },
     "user_tz": -180
    },
    "id": "cTaIYB3T8LrY",
    "outputId": "6ecb432b-8316-4622-d300-9227ab8f8e12"
   },
   "outputs": [],
   "source": [
    "rounded_labels=np.argmax(y_test, axis=1)\n",
    "rounded_labels[1]\n",
    "\n",
    "y_pred = model.predict(X_test, batch_size=64, verbose=1)\n",
    "y_pred_bool = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(classification_report(rounded_labels, y_pred_bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b177d6",
   "metadata": {
    "id": "23b177d6",
    "outputId": "32cf85d5-026d-4a51-8c68-96b544e4acef"
   },
   "outputs": [],
   "source": [
    "!mlflow ui"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417336c9",
   "metadata": {
    "id": "417336c9"
   },
   "source": [
    "# Save the model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211c1c26",
   "metadata": {
    "id": "211c1c26",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.save('models/9KasimTransformer_egekubi.h5') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3101ddb",
   "metadata": {
    "id": "c3101ddb"
   },
   "source": [
    "# Detect in real time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cff19d",
   "metadata": {
    "id": "99cff19d"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "mpHands = mp.solutions.hands #initiate the mpHands object with mediapipe\n",
    "hands = mpHands.Hands(static_image_mode=False, #set prediction parameters \n",
    "                      max_num_hands=2,\n",
    "                      min_detection_confidence=0.5,\n",
    "                      min_tracking_confidence=0.5)\n",
    "mpDraw = mp.solutions.drawing_utils #initiate the drawing object vor visualization\n",
    "\n",
    "#we specifically use the Hands part of the mediapipe detection pipeline, since we do not need face pose etc.\n",
    "#the output of mpHands.Hands is a dictionary consisting of 0 and 1, meaning the left hand and the right hand in order.\n",
    "#each hand consists of 21 keypoints, each containing 3 pixels (x,y,z) values.\n",
    "#the values are normalized according to the image width and height by default.\n",
    "\n",
    "#this function flattens all the data to an array for left and right hand and joins them together.\n",
    "#since our model will run based on two hands, we have to make sure we somehow fill all of the 126 features\n",
    "#our model requires.\n",
    "#to do this, we extract the first and last sub dictionary of the mediapipe output.\n",
    "#by doing this, if only one hand is present, the missing hand data will be duplicated as the present hand data.\n",
    "#the model will throw a shape mismatch error if all of the features are not present. \n",
    "def extract_keypoints(results):   \n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.multi_hand_landmarks[0].landmark]).flatten() if results.multi_hand_landmarks else np.zeros(21*3)   \n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.multi_hand_landmarks[-1].landmark]).flatten() if results.multi_hand_landmarks else np.zeros(21*3)   \n",
    "    \n",
    "    return np.concatenate([lh, rh])\n",
    "\n",
    "actions = np.array(['open', 'select' , 'close' , 'confirm' , 'reject' , #list of actions or classes to be predicted\n",
    "                    'increasevol' , 'decreasevol' , 'next' , 'previous'])\n",
    "n_classes = len(actions) #how many classes are there\n",
    "input_shape = (30,126)\n",
    "X_train = np.zeros((2,30,126))\n",
    "X_train.shape[1:]\n",
    "\n",
    "\n",
    "#This is the main transformer block\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs) #layer normalization is applied to the input to ease training\n",
    "    x = layers.MultiHeadAttention(    #This is the attention layer. You can experiment with different parameters \n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x) #as a result, apply dropout \n",
    "    res = x + inputs #residual connection\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res) #feed the result to layer normalization\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x) #use 1d convolution with kernel size 1 to preserve original sequence length\n",
    "    x = layers.Dropout(dropout)(x) #apply dropout\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x) #we need a filter size of the feature count so we can calculate weights\n",
    "    return x + res\n",
    "\n",
    "\n",
    "def build_model( #construct the model \n",
    "    input_shape,\n",
    "    head_size, #head size for the attention block\n",
    "    num_heads,#number of heads for the attention block\n",
    "    ff_dim, #feed forward dimensions\n",
    "    num_transformer_blocks, #specifies how many times the transformer block is repeated\n",
    "    mlp_units, #multilayer perceptron units\n",
    "    dropout=0, #by default\n",
    "    mlp_dropout=0, #by default\n",
    "):\n",
    "    inputs = keras.Input(shape=input_shape) #take the input\n",
    "    x = inputs   \n",
    "    \n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout) #feed the input to the transformer block\n",
    "    \n",
    "    \n",
    "    #apply global average pooling for classification\n",
    "    #be aware of the data format argument. If channels_last is specified, the dataset should be as\n",
    "    # (batch size,sequence length,number of features)\n",
    "    # if specified as channels first, sequence length and number feature dimensions are swapped.\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_last\")(x) \n",
    "    \n",
    "    \n",
    "    \n",
    "    for dim in mlp_units: #create the dense layer\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = layers.Dense(n_classes, activation=\"softmax\")(x) #apply softmax for probability distribution\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "input_shape = X_train.shape[1:] #take the last two arguments of the input shape which are 30,126\n",
    "\n",
    "model = build_model( #these are all hyperparameters for the model complexity. General rule of thumb\n",
    "    #would be two start as low as possible for all of them except the dropouts and gradually increase them until overfitting\n",
    "    input_shape,\n",
    "    head_size=128,\n",
    "    num_heads=4,\n",
    "    ff_dim=32,\n",
    "    num_transformer_blocks=1,\n",
    "    mlp_units=[16], #32\n",
    "    mlp_dropout=0, #0.4\n",
    "    dropout=0,\n",
    ")\n",
    "\n",
    "\n",
    "model.load_weights('models/11EylulTransformer.h5') \n",
    "#model.load_weights('models/7AgustosTransformer.h5') \n",
    "\n",
    "\n",
    "\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.75\n",
    "        \n",
    "cap = cv2.VideoCapture(1)       \n",
    "while cap.isOpened():\n",
    " \n",
    "    success, img = cap.read()\n",
    "    imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(imgRGB)\n",
    "    #print(results.multi_hand_landmarks)\n",
    "      \n",
    "     \n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        for handLms in results.multi_hand_landmarks:\n",
    "            for id, lm in enumerate(handLms.landmark):               \n",
    "                h, w, c = imgRGB.shape\n",
    "                cx, cy = int(lm.x *w), int(lm.y*h)                \n",
    "                cv2.circle(imgRGB, (cx,cy), 3, (255,0,255), cv2.FILLED)\n",
    "    \n",
    "            mpDraw.draw_landmarks(imgRGB, handLms, mpHands.HAND_CONNECTIONS)\n",
    "    \n",
    "    keypoints = extract_keypoints(results)\n",
    "    sequence.append(keypoints)\n",
    "    sequence = sequence[-30:]\n",
    "    \n",
    "    if len(sequence) == 30:\n",
    "        res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "        two_max = (-res).argsort()[:2] #find the two largest values of the softmax output\n",
    "        #if res[np.argmax(res)] > threshold:\n",
    "        if (res[two_max[0]] - res[two_max[1]])/res[two_max[0]] > threshold:  #if the normalized difference is greater, visualize  \n",
    "            #print(actions[np.argmax(res)])\n",
    "            #predictions.append(np.argmax(res))\n",
    "            cv2.putText(imgRGB, actions[np.argmax(res)], (3,30), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            cv2.putText(imgRGB, str(np.amax(res)), (3,80), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    \n",
    "        #sequence = []\n",
    "    cv2.imshow('OpenCV Feed', imgRGB)\n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "                \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e439d287",
   "metadata": {
    "id": "e439d287"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
